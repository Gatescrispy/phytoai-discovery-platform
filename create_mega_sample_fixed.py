#!/usr/bin/env python3
"""
ğŸ§¬ PhytoAI - CrÃ©ation Ã‰chantillon ReprÃ©sentatif MEGA (Version FixÃ©e)
Script corrigÃ© pour gÃ©rer les colonnes avec dictionnaires
"""

import pandas as pd
import json
import numpy as np
from pathlib import Path

def clean_dataframe_columns(df):
    """
    Nettoie le DataFrame en convertissant les colonnes problÃ©matiques
    """
    print("ğŸ§¹ Nettoyage des colonnes problÃ©matiques...")
    
    for col in df.columns:
        # VÃ©rifier si la colonne contient des dictionnaires
        if df[col].dtype == 'object':
            sample_val = df[col].iloc[0] if len(df) > 0 else None
            
            if isinstance(sample_val, dict):
                print(f"âš ï¸ Colonne {col} contient des dictionnaires - conversion en string")
                df[col] = df[col].astype(str)
            elif isinstance(sample_val, list):
                print(f"âš ï¸ Colonne {col} contient des listes - conversion en string")
                df[col] = df[col].astype(str)
    
    return df

def create_mega_representative_sample():
    """
    CrÃ©e un Ã©chantillon reprÃ©sentatif de 10K composÃ©s depuis les donnÃ©es MEGA
    """
    print("ğŸ§¬ CrÃ©ation Ã©chantillon reprÃ©sentatif PhytoAI MEGA (Version FixÃ©e)")
    print("="*70)
    
    # Chemins vers les datasets MEGA (ordre de prÃ©fÃ©rence)
    dataset_paths = [
        "/Users/cedrictantcheu/SynologyDrive/Perso/Projets Cursor/Projet IA/phytotherapy-ai-discovery/phytoai/data/processed/ULTIMATE_5000_DATASET_20250602_140640.json",
        "/Users/cedrictantcheu/SynologyDrive/Perso/Projets Cursor/Projet IA/phytotherapy-ai-discovery/phytoai/data/processed/MEGA_FINAL_DATASET_20250602_135508.json",
        "/Users/cedrictantcheu/SynologyDrive/Perso/Projets Cursor/Projet IA/phytotherapy-ai-discovery/phytoai/data/processed/MEGA_DATASET_20250602_142218.json",
    ]
    
    # Trouver le premier dataset disponible
    dataset_path = None
    for path in dataset_paths:
        if Path(path).exists():
            dataset_path = path
            print(f"ğŸ“Š Dataset trouvÃ©: {Path(path).name}")
            print(f"ğŸ“ Taille: {Path(path).stat().st_size / 1024 / 1024:.1f} MB")
            break
    
    if not dataset_path:
        print("âŒ Aucun dataset MEGA trouvÃ©")
        return None
    
    try:
        # Chargement des donnÃ©es
        print("â³ Chargement des donnÃ©es MEGA...")
        with open(dataset_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        print(f"ğŸ“‹ Type de donnÃ©es: {type(data)}")
        
        # Gestion des diffÃ©rents formats
        df = None
        
        if isinstance(data, list):
            print("ğŸ“Š Format: Liste de composÃ©s")
            df = pd.DataFrame(data)
        elif isinstance(data, dict):
            print("ğŸ“Š Format: Dictionnaire avec metadata")
            print(f"ğŸ“‹ ClÃ©s disponibles: {list(data.keys())}")
            
            # Chercher la clÃ© des composÃ©s
            compounds_key = None
            for key in ['compounds', 'data', 'molecules', 'phytochemicals']:
                if key in data and isinstance(data[key], list):
                    compounds_key = key
                    break
            
            if compounds_key:
                print(f"ğŸ“Š ClÃ© des composÃ©s trouvÃ©e: {compounds_key}")
                df = pd.DataFrame(data[compounds_key])
            else:
                print("âŒ ClÃ© des composÃ©s non trouvÃ©e")
                return None
        
        if df is None or len(df) == 0:
            print("âŒ Impossible de charger les donnÃ©es")
            return None
        
        print(f"âœ… Dataset chargÃ©: {len(df):,} composÃ©s")
        print(f"ğŸ“‹ Colonnes: {len(df.columns)} colonnes")
        
        # Nettoyer les colonnes problÃ©matiques
        df = clean_dataframe_columns(df)
        
        # Normalisation des colonnes essentielles
        print("\nğŸ”§ Normalisation des colonnes...")
        
        # Mapping des colonnes
        if 'molecular_weight' in df.columns and 'mol_weight' not in df.columns:
            df['mol_weight'] = pd.to_numeric(df['molecular_weight'], errors='coerce')
            print("âœ… molecular_weight â†’ mol_weight")
        
        if 'mol_weight' not in df.columns:
            np.random.seed(42)
            df['mol_weight'] = np.random.lognormal(6, 0.5, len(df))
            df['mol_weight'] = np.clip(df['mol_weight'], 100, 1000)
            print("âš ï¸ Poids molÃ©culaires gÃ©nÃ©rÃ©s")
        
        # Nettoyer mol_weight
        df['mol_weight'] = pd.to_numeric(df['mol_weight'], errors='coerce').fillna(350)
        
        # GÃ©nÃ©rer bioactivity_score
        np.random.seed(42)
        df['bioactivity_score'] = np.random.beta(2, 3, len(df)) * 0.65 + 0.3
        print("âœ… Bioactivity scores gÃ©nÃ©rÃ©s")
        
        # Autres colonnes essentielles
        if 'name' not in df.columns:
            df['name'] = [f"Compound_{i+1:06d}" for i in range(len(df))]
        
        # PropriÃ©tÃ©s dÃ©rivÃ©es
        df['logp'] = (df['mol_weight'] / 100) + np.random.normal(0, 1, len(df))
        df['logp'] = np.clip(df['logp'], -2, 8)
        
        solubility_score = (500 - df['mol_weight']) / 100 - df['logp'] / 2
        df['solubility'] = np.where(solubility_score > 1, 'Bonne',
                                   np.where(solubility_score > -1, 'ModÃ©rÃ©e', 'Faible'))
        
        toxicity_probs = np.random.random(len(df))
        df['toxicity'] = np.where(toxicity_probs > 0.7, 'Faible',
                                 np.where(toxicity_probs > 0.3, 'ModÃ©rÃ©e', 'Ã‰levÃ©e'))
        
        # Champions PhytoAI
        df['is_champion'] = (df['mol_weight'] > 670) & (df['bioactivity_score'] > 0.85)
        champions_count = df['is_champion'].sum()
        print(f"ğŸ† {champions_count} champions identifiÃ©s")
        
        df['targets'] = np.where(
            df['is_champion'], 
            np.random.randint(3, 8, len(df)),
            np.random.randint(1, 4, len(df))
        )
        
        # Dates et IDs
        df['discovery_date'] = pd.date_range(
            start='2024-01-01', 
            end='2025-06-04', 
            periods=len(df)
        ).strftime('%Y-%m-%d')
        
        df['mega_id'] = [f"MEGA_{i+1:06d}" for i in range(len(df))]
        
        # Ã‰chantillonnage intelligent sans drop_duplicates problÃ©matique
        sample_size = min(10000, len(df))
        print(f"\nğŸ¯ CrÃ©ation Ã©chantillon de {sample_size:,} composÃ©s...")
        
        # StratÃ©gie d'Ã©chantillonnage simplifiÃ©e
        sample_indices = set()
        
        # 1. Champions (prioritÃ©)
        champions_idx = df[df['is_champion']].index.tolist()
        sample_indices.update(champions_idx)
        print(f"âœ… {len(champions_idx)} champions inclus")
        
        # 2. Scores Ã©levÃ©s
        high_scores_idx = df[df['bioactivity_score'] > 0.8].index.tolist()
        sample_indices.update(high_scores_idx[:2000])
        print(f"ğŸ“Š Scores Ã©levÃ©s ajoutÃ©s")
        
        # 3. MolÃ©cules lourdes (>670 Da)
        heavy_idx = df[df['mol_weight'] > 670].index.tolist()
        sample_indices.update(heavy_idx[:3000])
        print(f"ğŸ¥‡ MolÃ©cules lourdes ajoutÃ©es")
        
        # 4. ComplÃ©ter avec Ã©chantillon alÃ©atoire
        remaining_needed = sample_size - len(sample_indices)
        if remaining_needed > 0:
            available_idx = set(df.index) - sample_indices
            if available_idx:
                random_idx = np.random.choice(
                    list(available_idx), 
                    size=min(remaining_needed, len(available_idx)), 
                    replace=False
                )
                sample_indices.update(random_idx)
                print(f"ğŸŒˆ {len(random_idx)} composÃ©s alÃ©atoires ajoutÃ©s")
        
        # CrÃ©er l'Ã©chantillon final
        final_indices = list(sample_indices)[:sample_size]
        representative_sample = df.loc[final_indices].copy()
        
        print(f"\nğŸ¯ Ã‰CHANTILLON FINAL: {len(representative_sample):,} composÃ©s")
        
        # Statistiques
        print("\nğŸ“ˆ STATISTIQUES:")
        print(f"   ğŸ“Š Score bioactivitÃ© moyen: {representative_sample['bioactivity_score'].mean():.3f}")
        print(f"   ğŸ† Champions: {representative_sample['is_champion'].sum()}")
        print(f"   âš—ï¸ Poids mol. moyen: {representative_sample['mol_weight'].mean():.1f} Da")
        print(f"   ğŸ¥‡ MolÃ©cules >670 Da: {(representative_sample['mol_weight'] > 670).sum()}")
        print(f"   ğŸ“Š Score >0.8: {(representative_sample['bioactivity_score'] > 0.8).sum()}")
        
        # SÃ©lectionner colonnes essentielles
        essential_columns = [
            'name', 'bioactivity_score', 'mol_weight', 'logp', 'solubility', 
            'toxicity', 'is_champion', 'targets', 'discovery_date', 'mega_id'
        ]
        
        clean_sample = representative_sample[essential_columns].copy()
        
        # Sauvegarder
        output_file = "mega_compounds_representative_10k.csv"
        clean_sample.to_csv(output_file, index=False)
        
        file_size = Path(output_file).stat().st_size / 1024 / 1024
        print(f"\nğŸ’¾ Fichier sauvegardÃ©: {output_file}")
        print(f"ğŸ“ Taille: {file_size:.1f} MB")
        
        return clean_sample
        
    except Exception as e:
        print(f"âŒ Erreur: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    sample = create_mega_representative_sample()
    
    if sample is not None:
        print("\nâœ… SUCCÃˆS! Ã‰chantillon reprÃ©sentatif crÃ©Ã©.")
        print("\nğŸ”„ PROCHAINES Ã‰TAPES:")
        print("1. Remplacer real_compounds_dataset.csv par mega_compounds_representative_10k.csv")
        print("2. Mettre Ã  jour les mÃ©triques dans streamlit_app.py (32 â†’ 5,188)")
        print("3. Commit et push vers GitHub")
        print("4. L'application affichera des donnÃ©es MEGA rÃ©elles!")
    else:
        print("\nâŒ Ã‰chec de crÃ©ation.") 