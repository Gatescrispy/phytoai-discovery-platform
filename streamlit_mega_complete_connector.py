#!/usr/bin/env python3
"""
üöÄ PhytoAI - Connecteur MEGA Complet 1.4M Mol√©cules
Streaming intelligent depuis dataset local ou Hugging Face
"""

import streamlit as st
import pandas as pd
import numpy as np
import os
from pathlib import Path
from datetime import datetime, timedelta
import random
import time

class MegaCompleteConnector:
    def __init__(self):
        # Chemins vers les datasets
        self.local_dataset_path = "../mega_1400k_dataset_local"
        self.hf_dataset_name = "phytoai/mega-phytotherapy-complete-1400k"
        self.fallback_path = "mega_streamlit_50k.csv"
        
        # Cache et √©tat
        self._dataset_cache = None
        self._dataset_type = None
        self._last_load_time = None
        
    @st.cache_data(ttl=3600)
    def load_complete_mega_dataset(_self):
        """Chargement intelligent du dataset complet 1.4M mol√©cules"""
        print("üîç Tentative de chargement dataset MEGA complet...")
        
        # 1. Tentative dataset local d'abord (plus rapide)
        if _self._try_load_local_dataset():
            return _self._dataset_cache, "üü¢ MEGA COMPLET LOCAL (1.4M mol√©cules)"
        
        # 2. Tentative Hugging Face
        if _self._try_load_huggingface_dataset():
            return _self._dataset_cache, "üü¢ MEGA COMPLET HUGGING FACE (1.4M mol√©cules)"
        
        # 3. Fallback vers 50K
        if _self._try_load_fallback_dataset():
            return _self._dataset_cache, "üü° MEGA √âCHANTILLON (50K mol√©cules)"
        
        # 4. √âchec complet
        return pd.DataFrame(), "‚ùå Aucun dataset disponible"
    
    def _try_load_local_dataset(self):
        """Tentative de chargement du dataset local"""
        try:
            local_path = Path(self.local_dataset_path)
            if local_path.exists():
                print(f"üìÇ Chargement dataset local: {local_path}")
                
                # Import conditionnel de datasets
                try:
                    from datasets import load_from_disk
                    dataset_dict = load_from_disk(str(local_path))
                    
                    # Combine tous les splits pour l'application
                    all_data = []
                    for split_name in ['train', 'validation', 'test']:
                        if split_name in dataset_dict:
                            split_df = dataset_dict[split_name].to_pandas()
                            split_df['dataset_split'] = split_name
                            all_data.append(split_df)
                    
                    if all_data:
                        combined_df = pd.concat(all_data, ignore_index=True)
                        self._dataset_cache = combined_df
                        self._dataset_type = "local_complete"
                        print(f"‚úÖ Dataset local charg√©: {len(combined_df):,} mol√©cules")
                        return True
                        
                except ImportError:
                    print("‚ö†Ô∏è Module 'datasets' non install√© pour le dataset local")
                    
        except Exception as e:
            print(f"‚ùå Erreur chargement local: {e}")
        
        return False
    
    def _try_load_huggingface_dataset(self):
        """Tentative de chargement depuis Hugging Face"""
        try:
            print(f"ü§ó Tentative chargement Hugging Face: {self.hf_dataset_name}")
            
            # Import conditionnel
            try:
                from datasets import load_dataset
                
                # Chargement streaming pour √©conomiser la m√©moire
                dataset = load_dataset(
                    self.hf_dataset_name,
                    streaming=True,  # Mode streaming pour gros datasets
                    trust_remote_code=True
                )
                
                # Conversion en DataFrame avec limitation intelligente
                sample_size = 100000  # 100K √©chantillon pour test
                train_sample = dataset['train'].take(sample_size)
                df_sample = pd.DataFrame(list(train_sample))
                
                if len(df_sample) > 0:
                    self._dataset_cache = df_sample
                    self._dataset_type = "huggingface_streaming"
                    print(f"‚úÖ Hugging Face streaming charg√©: {len(df_sample):,} mol√©cules (√©chantillon)")
                    return True
                    
            except ImportError:
                print("‚ö†Ô∏è Module 'datasets' non install√© pour Hugging Face")
            except Exception as e:
                print(f"‚ö†Ô∏è Dataset HF non disponible: {e}")
                
        except Exception as e:
            print(f"‚ùå Erreur Hugging Face: {e}")
        
        return False
    
    def _try_load_fallback_dataset(self):
        """Chargement du fallback 50K mol√©cules"""
        try:
            if os.path.exists(self.fallback_path):
                print(f"üìä Chargement fallback: {self.fallback_path}")
                df = pd.read_csv(self.fallback_path)
                self._dataset_cache = df
                self._dataset_type = "fallback_50k"
                print(f"‚úÖ Fallback charg√©: {len(df):,} mol√©cules")
                return True
        except Exception as e:
            print(f"‚ùå Erreur fallback: {e}")
        
        return False
    
    @st.cache_data(ttl=1800)
    def search_molecules(_self, search_term, limit=100):
        """Recherche avanc√©e dans le dataset complet"""
        dataset, status = _self.load_complete_mega_dataset()
        
        if dataset.empty:
            return pd.DataFrame(), "‚ùå Dataset indisponible"
        
        try:
            search_term_lower = search_term.lower()
            
            # Recherche multi-crit√®res
            mask = (
                dataset['name'].str.lower().str.contains(search_term_lower, na=False) |
                dataset.get('molecular_family', pd.Series([''] * len(dataset))).str.lower().str.contains(search_term_lower, na=False) |
                dataset.get('targets', pd.Series([''] * len(dataset))).str.lower().str.contains(search_term_lower, na=False)
            )
            
            results = dataset[mask].copy()
            
            if len(results) > 0:
                # Tri intelligent par pertinence
                results['search_relevance'] = (
                    results['name'].str.lower().str.count(search_term_lower) * 3 +
                    results.get('complexity_score', pd.Series([0] * len(results))) * 0.1 +
                    results.get('bioactivity_score', pd.Series([0] * len(results))) * 2
                )
                
                results = results.sort_values(
                    ['search_relevance', 'complexity_score'], 
                    ascending=[False, False]
                ).head(limit)
                
                return results, f"üéØ {len(results)} r√©sultats trouv√©s"
            else:
                return pd.DataFrame(), f"‚ùå Aucun r√©sultat pour '{search_term}'"
                
        except Exception as e:
            return pd.DataFrame(), f"‚ùå Erreur recherche: {e}"
    
    @st.cache_data(ttl=900)
    def get_random_molecules(_self, count=10, category=None):
        """S√©lection al√©atoire intelligente de mol√©cules"""
        dataset, status = _self.load_complete_mega_dataset()
        
        if dataset.empty:
            return pd.DataFrame(), "‚ùå Dataset indisponible"
        
        try:
            # Filtrage par cat√©gorie si sp√©cifi√©
            if category and category != "Toutes":
                if category == "Champions" and 'is_champion' in dataset.columns:
                    filtered_dataset = dataset[dataset['is_champion'] == True]
                elif category == "Haute Complexit√©" and 'complexity_score' in dataset.columns:
                    filtered_dataset = dataset[dataset['complexity_score'] > 20]
                elif category == "Drug-like" and 'molecular_weight' in dataset.columns:
                    filtered_dataset = dataset[
                        (dataset['molecular_weight'] > 150) & 
                        (dataset['molecular_weight'] < 500)
                    ]
                else:
                    filtered_dataset = dataset
            else:
                filtered_dataset = dataset
            
            if len(filtered_dataset) == 0:
                filtered_dataset = dataset  # Fallback
            
            # √âchantillonnage pond√©r√© par complexit√©
            if 'complexity_score' in filtered_dataset.columns:
                weights = np.exp(filtered_dataset['complexity_score'] / 10)  # Pond√©ration exponentielle
                weights = weights / weights.sum()  # Normalisation
                
                try:
                    sample_indices = np.random.choice(
                        filtered_dataset.index, 
                        size=min(count, len(filtered_dataset)), 
                        replace=False, 
                        p=weights
                    )
                    results = filtered_dataset.loc[sample_indices]
                except:
                    # Fallback simple si pond√©ration √©choue
                    results = filtered_dataset.sample(min(count, len(filtered_dataset)))
            else:
                results = filtered_dataset.sample(min(count, len(filtered_dataset)))
            
            return results, f"üé≤ {len(results)} mol√©cules d√©couvertes"
            
        except Exception as e:
            return pd.DataFrame(), f"‚ùå Erreur d√©couverte: {e}"
    
    @st.cache_data(ttl=3600)
    def get_dataset_statistics(_self):
        """Statistiques compl√®tes du dataset"""
        dataset, status = _self.load_complete_mega_dataset()
        
        if dataset.empty:
            return {}, "‚ùå Statistiques indisponibles"
        
        try:
            stats = {
                'total_molecules': len(dataset),
                'dataset_type': _self._dataset_type,
                'unique_families': dataset.get('molecular_family', pd.Series()).nunique(),
                'avg_molecular_weight': dataset.get('molecular_weight', pd.Series([0])).mean(),
                'avg_complexity': dataset.get('complexity_score', pd.Series([0])).mean(),
                'avg_bioactivity': dataset.get('bioactivity_score', pd.Series([0])).mean(),
            }
            
            # Statistiques sp√©cialis√©es
            if 'is_champion' in dataset.columns:
                stats['champion_molecules'] = len(dataset[dataset['is_champion'] == True])
            
            if 'molecular_weight' in dataset.columns:
                stats['drug_like_molecules'] = len(dataset[
                    (dataset['molecular_weight'] >= 150) & 
                    (dataset['molecular_weight'] <= 500)
                ])
                stats['large_molecules'] = len(dataset[dataset['molecular_weight'] > 670])
            
            if 'complexity_score' in dataset.columns:
                stats['high_complexity'] = len(dataset[dataset['complexity_score'] > 20])
            
            return stats, status
            
        except Exception as e:
            return {}, f"‚ùå Erreur statistiques: {e}"
    
    def get_dataset_status_widget(self):
        """Widget de statut pour la sidebar Streamlit"""
        stats, status = self.get_dataset_statistics()
        
        # Affichage du statut principal
        if "üü¢" in status:
            if "LOCAL" in status:
                st.sidebar.success("üöÄ MEGA COMPLET LOCAL")
                st.sidebar.info("üíæ Dataset 1.4M en cache local")
            else:
                st.sidebar.success("üöÄ MEGA COMPLET HUGGING FACE")
                st.sidebar.info("‚òÅÔ∏è Streaming depuis Hugging Face")
        elif "üü°" in status:
            st.sidebar.warning("üìä MEGA √âCHANTILLON")
            st.sidebar.info("üîÑ Mode fallback 50K mol√©cules")
        else:
            st.sidebar.error("‚ùå DATASET INDISPONIBLE")
            return
        
        # M√©triques d√©taill√©es
        if stats:
            col1, col2 = st.sidebar.columns(2)
            
            with col1:
                st.metric("üíä Mol√©cules", f"{stats.get('total_molecules', 0):,}")
                if stats.get('champion_molecules', 0) > 0:
                    st.metric("üèÜ Champions", f"{stats['champion_molecules']:,}")
            
            with col2:
                if stats.get('drug_like_molecules', 0) > 0:
                    st.metric("üíâ Drug-like", f"{stats['drug_like_molecules']:,}")
                if stats.get('large_molecules', 0) > 0:
                    st.metric("üî¨ Complexes", f"{stats['large_molecules']:,}")
            
            # Scores moyens
            if stats.get('avg_complexity', 0) > 0:
                st.sidebar.metric(
                    "üß¨ Complexit√© moyenne", 
                    f"{stats['avg_complexity']:.1f}/30"
                )
            
            if stats.get('avg_bioactivity', 0) > 0:
                st.sidebar.metric(
                    "‚ö° Bioactivit√© moyenne", 
                    f"{stats['avg_bioactivity']:.3f}"
                )

# Instance globale pour l'application
mega_complete_connector = MegaCompleteConnector() 